{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1K4c3wohsGjo4sSGZntepeOQGApfHfdom",
      "authorship_tag": "ABX9TyNwkpfQMGhgBc6yJ+saoSvd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ambilynanjilath/deep-learning-projects/blob/master/json_creating.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/json_creating.ipynb'   '/content/drive/MyDrive/Data_Scraping'"
      ],
      "metadata": {
        "id": "Q4mgkRTuh5uK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UXW0qky929kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"ambilybiju2408@gmail.com\"\n",
        "!git config --global user.name \"ambilynanjilath\"\n"
      ],
      "metadata": {
        "id": "EWSb8ZtviYHu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Data_Scraping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAX8wGoZi-MO",
        "outputId": "c58e0a50-6a0c-431f-f20f-1e938cccbbce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data_Scraping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQgoCEn0jEtW",
        "outputId": "68fe1a3b-68f4-474f-ea3e-df56db8fa730"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/drive/MyDrive/Data_Scraping/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add json_creating.ipynb\n"
      ],
      "metadata": {
        "id": "jUyEo95pjLSt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"initial commit\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lfayq1tjSoB",
        "outputId": "5e457fc2-a98f-4e78-e4f5-3a0131d4ea85"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[master (root-commit) a88ca8f] initial commit\n",
            " 1 file changed, 677 insertions(+)\n",
            " create mode 100644 json_creating.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keygen -t rsa -b 4096 -C \"ambilybiju2408@gmail.com\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeJ2HkClnLc9",
        "outputId": "b224a653-b3d4-4cb3-8c61-7105c3e035fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Enter file in which to save the key (/root/.ssh/id_rsa): \n",
            "Created directory '/root/.ssh'.\n",
            "Enter passphrase (empty for no passphrase): \n",
            "Enter same passphrase again: \n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:ZXtne3OnPX4AIYmLGB8sTclm5U2Yz2OhFlscLklZiIU ambilybiju2408@gmail.com\n",
            "The key's randomart image is:\n",
            "+---[RSA 4096]----+\n",
            "|     =.*+O+o     |\n",
            "|    o EoB=* .    |\n",
            "|     B o+O=o .   |\n",
            "|    . o =+=..    |\n",
            "|       .S.....o  |\n",
            "|           . o.. |\n",
            "|              .o+|\n",
            "|               ==|\n",
            "|              o.+|\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!eval \"$(ssh-agent -s)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APDAYtp3nMlL",
        "outputId": "ad2e5c84-8722-44f3-c253-fd20e1644fba"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent pid 13264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-add ~/.ssh/id_rsa\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRhHq4NRn5LL",
        "outputId": "edfff380-63fb-4431-bc8c-e155570a9924"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not open a connection to your authentication agent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-agent -k && ssh-agent -s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n2s3wh-oEJL",
        "outputId": "8b0c4938-a832-4d87-c85b-060ecce73c27"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSH_AGENT_PID not set, cannot kill agent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2Z-JpH3oSG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rcg8sFekoSD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhaGE-jRnMiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SCO5twflnMfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XQF8arWznMck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bestest best"
      ],
      "metadata": {
        "id": "_-b9PNou6iEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/YC_Companies(name,loc,tag).csv')\n"
      ],
      "metadata": {
        "id": "b3zeWT7Y9Rzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls=df['link'].to_list()"
      ],
      "metadata": {
        "id": "rRLAXn0g9jZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "def scrape_data(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Scrape company name\n",
        "        company_name = soup.find('h1', class_='font-extralight').text.strip()\n",
        "\n",
        "        # Check condition 1\n",
        "        if soup.find('div', class_='space-y-5'):\n",
        "            founders = []\n",
        "            founders_divs = soup.find_all('div', class_='flex flex-row flex-col items-start gap-3 md:flex-row')\n",
        "            for founder_div in founders_divs:\n",
        "                founder_info = {'name': '', 'role': '', 'description': '', 'linkedin_profile': '', 'twitter_profile': ''}\n",
        "                name_role_desc_div = founder_div.find('div', class_='flex-grow')\n",
        "                if name_role_desc_div:\n",
        "                    name_role = name_role_desc_div.find('h3', class_='text-lg font-bold')\n",
        "                    description = name_role_desc_div.find('p', class_='prose max-w-full whitespace-pre-line')\n",
        "                    if name_role:\n",
        "                        name_role_parts = name_role.text.split(',', 1)\n",
        "                        if len(name_role_parts) >= 2:\n",
        "                            founder_info['name'] = name_role_parts[0].strip()\n",
        "                            founder_info['role'] = name_role_parts[1].strip()\n",
        "                        elif len(name_role_parts) == 1:\n",
        "                            founder_info['name'] = name_role_parts[0].strip()\n",
        "                    if description:\n",
        "                        founder_info['description'] = description.text.strip()\n",
        "\n",
        "                linkedin_twitter_div = founder_div.find('div', class_='shrink-0 space-y-1.5 rounded-md border-[1px] border-[#999] bg-[#FDFDF7] p-6 sm:w-[300px]')\n",
        "                if linkedin_twitter_div:\n",
        "                    social_links = linkedin_twitter_div.find('div', class_='mt-1 space-x-2')\n",
        "                    linkedin_tag = social_links.find('a', class_='bg-image-linkedin')\n",
        "                    twitter_tag = social_links.find('a', class_='bg-image-twitter')\n",
        "                    if linkedin_tag:\n",
        "                        founder_info['linkedin_profile'] = linkedin_tag['href']\n",
        "                    if twitter_tag:\n",
        "                        founder_info['twitter_profile'] = twitter_tag['href']\n",
        "\n",
        "                founders.append(founder_info)\n",
        "\n",
        "            return {'company': {'name': company_name}, 'founders': founders}\n",
        "\n",
        "        # Check condition 2\n",
        "        elif soup.find('div', class_='space-y-4'):\n",
        "            founders = []\n",
        "            founder_divs = soup.find_all('div', class_='shrink-0 space-y-1.5 rounded-md border-[1px] border-[#999] bg-[#FDFDF7] p-6 sm:w-[300px]')\n",
        "            for founder_div in founder_divs:\n",
        "                founder_info = {'name': '', 'role': '', 'description': '', 'linkedin_profile': '', 'twitter_profile': ''}\n",
        "                name = founder_div.find('div', class_='font-bold').text.strip()\n",
        "                role = founder_div.find('div').text.strip()\n",
        "                linkedin_tag = founder_div.find('a', class_='bg-image-linkedin')\n",
        "                twitter_tag = founder_div.find('a', class_='bg-image-twitter')\n",
        "                founder_info['name'] = name\n",
        "                founder_info['role'] = role\n",
        "                if linkedin_tag:\n",
        "                    founder_info['linkedin_profile'] = linkedin_tag['href']\n",
        "                if twitter_tag:\n",
        "                    founder_info['twitter_profile'] = twitter_tag['href']\n",
        "                founders.append(founder_info)\n",
        "\n",
        "            return {'company': {'name': company_name}, 'founders': founders}\n",
        "\n",
        "        else:\n",
        "            # If neither condition 1 nor condition 2 is satisfied\n",
        "            return {'company': {'name': company_name}, 'founders': []}\n",
        "    else:\n",
        "        print(\"Failed to fetch page:\", url)\n",
        "        return None\n",
        "\n",
        "\n",
        "data = []\n",
        "for url in urls:\n",
        "    result = scrape_data(url)\n",
        "    if result:\n",
        "        data.append(result)\n",
        "\n",
        "# Write data to JSON file\n",
        "with open('founder_data.json', 'w') as f:\n",
        "    json.dump(data, f, indent=4)\n",
        "\n",
        "print(\"Data scraped and saved to 'scraped_data.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_KXAnq06JSk",
        "outputId": "f3470561-ff90-4316-d833-3c2997128ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data scraped and saved to 'scraped_data.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKlf0aW_9Pn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OCAG4t7B9PlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load data from Company_data.json\n",
        "with open('Company_data.json', 'r') as company_file:\n",
        "    company_data = json.load(company_file)\n",
        "\n",
        "# Load data from founder_data.json\n",
        "with open('founder_data.json', 'r') as founder_file:\n",
        "    founder_data = json.load(founder_file)\n",
        "\n",
        "# Initialize a dictionary to store combined data\n",
        "combined_data = {}\n",
        "\n",
        "# Extract company information\n",
        "for company_entry in company_data['companies']:\n",
        "    company_name = company_entry['name']\n",
        "    combined_data['company'] = {\n",
        "        \"name\": company_name,\n",
        "        \"tagline\": company_entry['tagline'],\n",
        "        \"description\": company_entry['description'],\n",
        "        \"batch\": company_entry['batch'],\n",
        "        \"company_type\": company_entry['company_type'],\n",
        "        \"industry_tags\": company_entry['industry_tags'],\n",
        "        \"location\": company_entry['location'],\n",
        "        \"website\": company_entry['website'],\n",
        "        \"founded\": company_entry['founded'],\n",
        "        \"team_size\": company_entry['team_size'],\n",
        "        \"social_profiles\": company_entry['social_profiles']\n",
        "    }\n",
        "\n",
        "# Extract founder information\n",
        "for founder_entry in founder_data:\n",
        "    if founder_entry['company']['name'] == combined_data['company']['name']:\n",
        "        combined_data['founders'] = []\n",
        "        for founder in founder_entry['founders']:\n",
        "            combined_data['founders'].append({\n",
        "                \"name\": founder['name'],\n",
        "                \"description\": founder['description'],\n",
        "                \"twitter_profile\": founder['twitter_profile'],\n",
        "                \"linkedin_profile\": founder['linkedin_profile']\n",
        "            })\n",
        "\n",
        "# Write combined data to a new JSON file\n",
        "with open('yc_companies.json', 'w') as combined_file:\n",
        "    json.dump(combined_data, combined_file, indent=4)\n"
      ],
      "metadata": {
        "id": "BtUuly5J9Pid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bHgk8pzG9Pf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aGPJ4o9r9PdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lt7YDGii9Pap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load data from Company_data.json\n",
        "with open('Company_data.json', 'r') as company_file:\n",
        "    company_data = json.load(company_file)\n",
        "\n",
        "# Load data from founder_data.json\n",
        "with open('founder_data.json', 'r') as founder_file:\n",
        "    founder_data = json.load(founder_file)\n",
        "\n",
        "# Initialize a list to store combined data for multiple companies\n",
        "combined_data_list = []\n",
        "\n",
        "# Extract company and founder information\n",
        "for company_entry in company_data['companies']:\n",
        "    company_name = company_entry['name']\n",
        "    combined_company_data = {\n",
        "        \"company\": {\n",
        "            \"name\": company_name,\n",
        "            \"tagline\": company_entry['tagline'],\n",
        "            \"description\": company_entry['description'],\n",
        "            \"batch\": company_entry['batch'],\n",
        "            \"company_type\": company_entry['company_type'],\n",
        "            \"industry_tags\": company_entry['industry_tags'],\n",
        "            \"location\": company_entry['location'],\n",
        "            \"website\": company_entry['website'],\n",
        "            \"founded\": company_entry['founded'],\n",
        "            \"team_size\": company_entry['team_size'],\n",
        "            \"social_profiles\": company_entry['social_profiles']\n",
        "        },\n",
        "        \"founders\": []\n",
        "    }\n",
        "\n",
        "    for founder_entry in founder_data:\n",
        "        if founder_entry['company']['name'] == company_name:\n",
        "            for founder in founder_entry['founders']:\n",
        "                combined_company_data['founders'].append({\n",
        "                    \"name\": founder['name'],\n",
        "                    \"description\": founder['description'],\n",
        "                    \"twitter_profile\": founder['twitter_profile'],\n",
        "                    \"linkedin_profile\": founder['linkedin_profile']\n",
        "                })\n",
        "\n",
        "    combined_data_list.append(combined_company_data)\n",
        "\n",
        "# Write combined data to a new JSON file\n",
        "with open('YC_Company.json', 'w') as combined_file:\n",
        "    json.dump(combined_data_list, combined_file, indent=4)\n"
      ],
      "metadata": {
        "id": "RMjBd_BM9PX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON file\n",
        "with open('/content/YC_Company.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Count the elements\n",
        "num_elements = len(data)\n",
        "\n",
        "print(\"Number of elements in the JSON file:\", num_elements)\n"
      ],
      "metadata": {
        "id": "KnYLnffB9PVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f073a6-8a63-4f27-b921-d34e422b7b47"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of elements in the JSON file: 4666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k1QioGLV9PSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lIW3ZbF9PQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ur-Aleh9PNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KjZhDZUk9PLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MDGO8tpv9PId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#best1"
      ],
      "metadata": {
        "id": "D1RxroqD293A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "def scrape_data(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Scrape company name\n",
        "        company_name = soup.find('h1', class_='font-extralight').text.strip()\n",
        "\n",
        "        # Check condition 1\n",
        "        if soup.find('div', class_='space-y-5'):\n",
        "            founders = []\n",
        "            founders_divs = soup.find_all('div', class_='flex flex-row flex-col items-start gap-3 md:flex-row')\n",
        "            for founder_div in founders_divs:\n",
        "                founder_info = {}\n",
        "                name_role_desc_div = founder_div.find('div', class_='flex-grow')\n",
        "                if name_role_desc_div:\n",
        "                    name_role = name_role_desc_div.find('h3', class_='text-lg font-bold')\n",
        "                    description = name_role_desc_div.find('p', class_='prose max-w-full whitespace-pre-line')\n",
        "                    if name_role:\n",
        "                        name_role_parts = name_role.text.split(',', 1)\n",
        "                        if len(name_role_parts) >= 2:\n",
        "                            founder_info['name'] = name_role_parts[0].strip()\n",
        "                            founder_info['role'] = name_role_parts[1].strip()\n",
        "                        elif len(name_role_parts) == 1:\n",
        "                            founder_info['name'] = name_role_parts[0].strip()\n",
        "                            founder_info['role'] = ''\n",
        "                    if description:\n",
        "                        founder_info['description'] = description.text.strip()\n",
        "\n",
        "                linkedin_twitter_div = founder_div.find('div', class_='shrink-0 space-y-1.5 rounded-md border-[1px] border-[#999] bg-[#FDFDF7] p-6 sm:w-[300px]')\n",
        "                if linkedin_twitter_div:\n",
        "                    social_links = linkedin_twitter_div.find('div', class_='mt-1 space-x-2')\n",
        "                    linkedin_tag = social_links.find('a', class_='bg-image-linkedin')\n",
        "                    twitter_tag = social_links.find('a', class_='bg-image-twitter')\n",
        "                    if linkedin_tag:\n",
        "                        founder_info['linkedin_profile'] = linkedin_tag['href']\n",
        "                    if twitter_tag:\n",
        "                        founder_info['twitter_profile'] = twitter_tag['href']\n",
        "\n",
        "                founders.append(founder_info)\n",
        "\n",
        "            return {'company': {'name': company_name}, 'founders': founders}\n",
        "\n",
        "        # Check condition 2\n",
        "        elif soup.find('div', class_='space-y-4'):\n",
        "            founders = []\n",
        "            founder_divs = soup.find_all('div', class_='shrink-0 space-y-1.5 rounded-md border-[1px] border-[#999] bg-[#FDFDF7] p-6 sm:w-[300px]')\n",
        "            for founder_div in founder_divs:\n",
        "                founder_info = {}\n",
        "                name = founder_div.find('div', class_='font-bold').text.strip()\n",
        "                role = founder_div.find('div').text.strip()\n",
        "                linkedin_tag = founder_div.find('a', class_='bg-image-linkedin')\n",
        "                twitter_tag = founder_div.find('a', class_='bg-image-twitter')\n",
        "                founder_info['name'] = name\n",
        "                founder_info['role'] = role\n",
        "                if linkedin_tag:\n",
        "                    founder_info['linkedin_profile'] = linkedin_tag['href']\n",
        "                if twitter_tag:\n",
        "                    founder_info['twitter_profile'] = twitter_tag['href']\n",
        "                founders.append(founder_info)\n",
        "\n",
        "            return {'company': {'name': company_name}, 'founders': founders}\n",
        "\n",
        "        else:\n",
        "            # If neither condition 1 nor condition 2 is satisfied\n",
        "            return {'company': {'name': company_name}, 'founders': []}\n",
        "    else:\n",
        "        print(\"Failed to fetch page:\", url)\n",
        "        return None\n",
        "\n",
        "urls = [\n",
        "    \"https://www.ycombinator.com/companies/homeflow\",\n",
        "    \"https://www.ycombinator.com/companies/finta\",\n",
        "    \"https://www.ycombinator.com/companies/intercept\",\n",
        "    \"https://www.ycombinator.com/companies/fern\",\n",
        "    \"https://www.ycombinator.com/companies/axle\",\n",
        "    \"https://www.ycombinator.com/companies/hona\",\n",
        "    \"https://www.ycombinator.com/companies/quivr\",\n",
        "    \"https://www.ycombinator.com/companies/capsule\",\n",
        "    \"https://www.ycombinator.com/companies/stralis-aircraft\",\n",
        "    \"https://www.ycombinator.com/companies/flex\",\n",
        "    \"https://www.ycombinator.com/companies/corefin\",\n",
        "    \"https://www.ycombinator.com/companies/affinity\",\n",
        "    \"https://www.ycombinator.com/companies/thera\",\n",
        "    \"https://www.ycombinator.com/companies/craze\",\n",
        "    \"https://www.ycombinator.com/companies/decohere\"\n",
        "]\n",
        "\n",
        "\n",
        "data = []\n",
        "for url in urls:\n",
        "    result = scrape_data(url)\n",
        "    if result:\n",
        "        data.append(result)\n",
        "\n",
        "# Write data to JSON file\n",
        "with open('scraped_data.json', 'w') as f:\n",
        "    json.dump(data, f, indent=4)\n",
        "\n",
        "print(\"Data scraped and saved to 'scraped_data.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjofcJGisdFI",
        "outputId": "b715f736-ab24-4771-fe84-ec88859ed8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data scraped and saved to 'scraped_data.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n6v6sx1l3DNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "def scrape_data(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Scrape company name\n",
        "        company_name = soup.find('h1', class_='font-extralight').text.strip()\n",
        "\n",
        "        # Check condition 1\n",
        "        if soup.find('div', class_='space-y-5'):\n",
        "            founders = []\n",
        "            founders_divs = soup.find_all('div', class_='flex flex-row flex-col items-start gap-3 md:flex-row')\n",
        "            for founder_div in founders_divs:\n",
        "                founder_info = {}\n",
        "                name_role_desc_div = founder_div.find('div', class_='flex-grow')\n",
        "                if name_role_desc_div:\n",
        "                    name_role = name_role_desc_div.find('h3', class_='text-lg font-bold')\n",
        "                    description = name_role_desc_div.find('p', class_='prose max-w-full whitespace-pre-line')\n",
        "                    if name_role:\n",
        "                        name_role_parts = name_role.text.split(',', 1)\n",
        "                        if len(name_role_parts) >= 2:\n",
        "                            founder_info['name'] = name_role_parts[0].strip()\n",
        "                            founder_info['role'] = name_role_parts[1].strip()  # Extract role here\n",
        "                        elif len(name_role_parts) == 1:\n",
        "                            founder_info['name'] = name_role_parts[0].strip()\n",
        "                            founder_info['role'] = ''\n",
        "                    if description:\n",
        "                        founder_info['description'] = description.text.strip()\n",
        "\n",
        "                linkedin_twitter_div = founder_div.find('div', class_='shrink-0 space-y-1.5 rounded-md border-[1px] border-[#999] bg-[#FDFDF7] p-6 sm:w-[300px]')\n",
        "                if linkedin_twitter_div:\n",
        "                    social_links = linkedin_twitter_div.find('div', class_='mt-1 space-x-2')\n",
        "                    linkedin_tag = social_links.find('a', class_='bg-image-linkedin')\n",
        "                    twitter_tag = social_links.find('a', class_='bg-image-twitter')\n",
        "                    if linkedin_tag:\n",
        "                        founder_info['linkedin_profile'] = linkedin_tag['href']\n",
        "                    if twitter_tag:\n",
        "                        founder_info['twitter_profile'] = twitter_tag['href']\n",
        "\n",
        "                founders.append(founder_info)\n",
        "\n",
        "            return {'company': {'name': company_name}, 'founders': founders}\n",
        "\n",
        "        # Check condition 2\n",
        "        elif soup.find('div', class_='space-y-4'):\n",
        "            founders = []\n",
        "            founder_divs = soup.find_all('div', class_='shrink-0 space-y-1.5 rounded-md border-[1px] border-[#999] bg-[#FDFDF7] p-6 sm:w-[300px]')\n",
        "            for founder_div in founder_divs:\n",
        "                founder_info = {}\n",
        "                name_role_div = founder_div.find('h3', class_='text-lg font-bold')\n",
        "                if name_role_div:\n",
        "                    name_role_parts = name_role_div.text.split(',', 1)\n",
        "                    if len(name_role_parts) >= 2:\n",
        "                        founder_info['name'] = name_role_parts[0].strip()\n",
        "                        founder_info['role'] = name_role_parts[1].strip()  # Extract role here\n",
        "                    else:\n",
        "                        founder_info['name'] = name_role_div.text.strip()\n",
        "                        founder_info['role'] = ''  # No role found, set to empty string\n",
        "                linkedin_tag = founder_div.find('a', class_='bg-image-linkedin')\n",
        "                twitter_tag = founder_div.find('a', class_='bg-image-twitter')\n",
        "                if linkedin_tag:\n",
        "                    founder_info['linkedin_profile'] = linkedin_tag['href']\n",
        "                if twitter_tag:\n",
        "                    founder_info['twitter_profile'] = twitter_tag['href']\n",
        "                founders.append(founder_info)\n",
        "\n",
        "            return {'company': {'name': company_name}, 'founders': founders}\n",
        "\n",
        "        else:\n",
        "            # If neither condition 1 nor condition 2 is satisfied\n",
        "            return {'company': {'name': company_name}, 'founders': []}\n",
        "    else:\n",
        "        print(\"Failed to fetch page:\", url)\n",
        "        return None\n",
        "\n",
        "urls = [\n",
        "    \"https://www.ycombinator.com/companies/homeflow\",\n",
        "    \"https://www.ycombinator.com/companies/finta\",\n",
        "    \"https://www.ycombinator.com/companies/intercept\",\n",
        "    \"https://www.ycombinator.com/companies/fern\",\n",
        "    \"https://www.ycombinator.com/companies/axle\",\n",
        "    \"https://www.ycombinator.com/companies/hona\",\n",
        "    \"https://www.ycombinator.com/companies/quivr\",\n",
        "    \"https://www.ycombinator.com/companies/capsule\",\n",
        "    \"https://www.ycombinator.com/companies/stralis-aircraft\",\n",
        "    \"https://www.ycombinator.com/companies/flex\",\n",
        "    \"https://www.ycombinator.com/companies/corefin\",\n",
        "    \"https://www.ycombinator.com/companies/affinity\",\n",
        "    \"https://www.ycombinator.com/companies/thera\",\n",
        "    \"https://www.ycombinator.com/companies/craze\",\n",
        "    \"https://www.ycombinator.com/companies/decohere\"\n",
        "]\n",
        "\n",
        "data = []\n",
        "for url in urls:\n",
        "    result = scrape_data(url)\n",
        "    if result:\n",
        "        data.append(result)\n",
        "\n",
        "# Write data to JSON file\n",
        "with open('scraped_data.json', 'w') as f:\n",
        "    json.dump(data, f, indent=4)\n",
        "\n",
        "print(\"Data scraped and saved to 'scraped_data.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GvW2elK3DKf",
        "outputId": "aafb1a6d-3e0b-4a1c-b2da-5b618c4d5604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data scraped and saved to 'scraped_data.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gf60xDYhqkHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dPRWFAq9qkEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IREGuKQ9qkBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Py4jqOitqja7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################OPENING THE WEBSITE####################################################\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import time\n",
        "\n",
        "chrome_options = Options()\n",
        "chrome_options.add_experimental_option(\"detach\", True)\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "driver.maximize_window()\n",
        "driver.get(\"https://www.ycombinator.com/companies\")\n",
        "\n",
        "# Wait for 2 minutes (120 seconds)\n",
        "time.sleep(120)\n",
        "\n",
        "# Close the browser window\n",
        "driver.quit()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QRZsYn1xqjYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "##############################OPENING WEBSITE INSIDE WEBSITE######################################\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import time\n",
        "\n",
        "chrome_options = Options()\n",
        "chrome_options.add_experimental_option(\"detach\", True)\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "driver.maximize_window()\n",
        "driver.get(\"https://www.ycombinator.com/companies\")\n",
        "\n",
        "# Wait for the page to load\n",
        "time.sleep(30)  # You can adjust this time if needed\n",
        "\n",
        "# Define the class name of the element to click\n",
        "element_class_name =\"_coName_99gj3_454\"\n",
        "\n",
        "# Wait for the element to be clickable\n",
        "wait = WebDriverWait(driver, 60)  # Maximum wait time of 60 seconds\n",
        "element = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, element_class_name)))\n",
        "\n",
        "# Click on the element\n",
        "element.click()\n",
        "\n",
        "# Wait for 2 minutes (120 seconds)\n",
        "time.sleep(120)\n",
        "\n",
        "# Close the browser window\n",
        "driver.quit()\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "lz0wKI7oqzPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##################### CLICKING AND GO BACK###############################################\n",
        "\n",
        "\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import time\n",
        "\n",
        "chrome_options = Options()\n",
        "chrome_options.add_experimental_option(\"detach\", True)\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "driver.maximize_window()\n",
        "driver.get(\"https://www.ycombinator.com/companies\")\n",
        "\n",
        "# Wait for the page to load (adjust if needed)\n",
        "time.sleep(30)\n",
        "\n",
        "# Define the class name of the element to click\n",
        "element_class_name = \"_coName_99gj3_454\"  # Update with actual class name\n",
        "\n",
        "# Wait for the element to be clickable\n",
        "wait = WebDriverWait(driver, 60)\n",
        "element = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, element_class_name)))\n",
        "\n",
        "# Click on the element\n",
        "element.click()\n",
        "\n",
        "# Go back to the previous page\n",
        "driver.back()\n",
        "\n",
        "# Wait for 15 seconds after going back\n",
        "time.sleep(15)\n",
        "\n",
        "# Close the browser window\n",
        "driver.quit()\n",
        "\n"
      ],
      "metadata": {
        "id": "9MDbZ34Rq2CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPLwQLE7rmr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3lEPh-g3rmpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "####################### clicking multiple company ############################################\n",
        "\n",
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.chrome.service import Service\n",
        "# from selenium.webdriver.chrome.options import Options\n",
        "# from selenium.webdriver.common.by import By\n",
        "# from selenium.webdriver.support.ui import WebDriverWait\n",
        "# from selenium.webdriver.support import expected_conditions as EC\n",
        "# from webdriver_manager.chrome import ChromeDriverManager\n",
        "# import time\n",
        "\n",
        "# # Increase wait times as needed for slower internet or complex pages\n",
        "# wait_time_for_load = 10\n",
        "# wait_time_per_company = 2\n",
        "\n",
        "# chrome_options = Options()\n",
        "# chrome_options.add_experimental_option(\"detach\", True)\n",
        "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "# driver.maximize_window()\n",
        "# driver.get(\"https://www.ycombinator.com/companies\")\n",
        "\n",
        "# # Wait for the page to load with expected conditions (preferred over time.sleep)\n",
        "# wait = WebDriverWait(driver, wait_time_for_load)\n",
        "# wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"_coName_99gj3_454\")))  # Update if needed\n",
        "\n",
        "# # Find elements with the class name (replace with the actual class)\n",
        "# company_elements = driver.find_elements(By.CLASS_NAME, \"_coName_99gj3_454\")\n",
        "\n",
        "# # Keep track of visited websites\n",
        "# visited_count = 0\n",
        "\n",
        "# for element in company_elements:\n",
        "#   # Click on the element\n",
        "#   element.click()\n",
        "#   visited_count += 1  # Increment counter on each visit\n",
        "\n",
        "#   # Wait for the company page to load (adjust if needed)\n",
        "#   time.sleep(wait_time_for_load)\n",
        "\n",
        "#   # Go back to the previous page (Y Combinator companies list)\n",
        "#   driver.back()\n",
        "\n",
        "#   # Wait after going back\n",
        "#   time.sleep(wait_time_per_company)\n",
        "\n",
        "# # Print the total count after processing\n",
        "# print(f\"Total websites visited: {visited_count}\")\n",
        "\n",
        "# # Close the browser window\n",
        "# driver.quit()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###################### NAME SCRAPING ######################\n",
        "\n",
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.chrome.service import Service\n",
        "# from selenium.webdriver.chrome.options import Options\n",
        "# from selenium.webdriver.common.by import By\n",
        "# from selenium.webdriver.support.ui import WebDriverWait\n",
        "# from selenium.webdriver.support import expected_conditions as EC\n",
        "# from webdriver_manager.chrome import ChromeDriverManager\n",
        "# import time\n",
        "\n",
        "# # Increase wait times as needed for slower internet or complex pages\n",
        "# wait_time_for_load = 10\n",
        "# wait_time_per_company = 2\n",
        "\n",
        "# chrome_options = Options()\n",
        "# chrome_options.add_experimental_option(\"detach\", True)\n",
        "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "# driver.maximize_window()\n",
        "# driver.get(\"https://www.ycombinator.com/companies\")\n",
        "\n",
        "# # Wait for the page to load with expected conditions\n",
        "# wait = WebDriverWait(driver, wait_time_for_load)\n",
        "# wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"_coName_99gj3_454\")))  # Update if needed\n",
        "\n",
        "# # Find company elements\n",
        "# company_elements = driver.find_elements(By.CLASS_NAME, \"_coName_99gj3_454\")\n",
        "\n",
        "# # Keep track of visited websites and scraped names\n",
        "# visited_count = 0\n",
        "# scraped_names = []\n",
        "\n",
        "# for element in company_elements:\n",
        "#   # Click on the element\n",
        "#   element.click()\n",
        "#   visited_count += 1\n",
        "\n",
        "#   # Wait for the company page to load (adjust if needed)\n",
        "#   time.sleep(wait_time_for_load)\n",
        "\n",
        "#   try:\n",
        "#     # Find the element with the company name (update class name if needed)\n",
        "#     company_name_element = driver.find_element(By.CLASS_NAME, \"font-extralight\")\n",
        "#     # Get the text content of the element\n",
        "#     company_name = company_name_element.text.strip()\n",
        "#     scraped_names.append(company_name)\n",
        "#     print(f\"Scraped name: {company_name}\")\n",
        "#   except:\n",
        "#     print(f\"Couldn't find company name element for company {visited_count}\")\n",
        "\n",
        "#   # Go back to the previous page\n",
        "#   driver.back()\n",
        "\n",
        "#   # Wait after going back\n",
        "#   time.sleep(wait_time_per_company)\n",
        "\n",
        "# # Print the total count after processing\n",
        "# print(f\"Total companies visited: {visited_count}\")\n",
        "# print(f\"Total names scraped: {len(scraped_names)}\")\n",
        "\n",
        "# # Close the browser window\n",
        "# driver.quit()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I-i2Qrf1rmVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}